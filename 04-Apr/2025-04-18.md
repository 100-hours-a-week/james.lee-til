# 날짜: 2025-04-18

## 진행한 내용
### 주제 1: AI Framework 조사
#### vLLM
- PagedAttention: GPU의 메모리를 '페이지'로 나누어, 여러 사용자의 요청을 병렬 처리
- Continuous Batching: 들어오는 요청을 기존 Batch의 빈자리에 합류시켜, GPU 활용률을 100%에 가깝게 함
- 구조가 복잡해서 설정 및 커스터마이징에는 난이도가 있다

#### Ollama
- 로컬에서 LLM을 쉽게 실행할 수 있도록 하는 플랫폼
- 로컬 특화라 실험용으로 사용했고, 웹 서빙에는 어려움도 있음

#### llama.cpp
- PyTorch와 달리, C++ 기반 프레임워크라 메모리 효율적
- CPU 최적화, 모델 양자화 등으로 가볍다
- 그만큼 무거운 모델 활용 어렵고, 성능 품질은 떨어질 수도

#### Hugging Face
- 다양한 AI 모델과 데이터셋을 제공하는 커뮤니티 중심 플랫폼
- KoBART 등 한국어 특화 모델 존재, Fine-tuning도 가능
- 서빙을 위해서는 여러 부가 작업이 필요할 수 있다
- Inference API는 유료

---

### 주제 2: Model 조사
#### EXAONE
- LG에서 공개한, 한국어 특화 경량 Language Model
- vLLM에 올릴 수 있으며, 품질과 속도 모두 좋다

#### llama 계열
- 속도가 빨라서 좋다
- 그러나 다국어 모델이라, 한국어 사이에 영어, 한자 등이 뒤섞임

---

### 주제 3: 아키텍처 모듈화
- 새로운 기능을 추가할 때, 기존 코드 수정이 아니라 추가만 하면 된다
- 기능마다 모듈을 분리하여, 하나의 모듈은 하나의 역할에만 집중, 유지보수가 편해진다

#### config
- API KEY, 모델 경로 등 민감 정보와 설정값을 중앙에서 관리
- 환경변수, .env, 또는 하드코딩 관련 내용 모두 여기로 통합

#### api
- 실제 FastAPI의 라우터들을 구성
- timeline, /merge, /hot 요청을 받아 처리

#### scrapers
- 각 API 별 스크래핑 함수를 모듈화
- 모듈 내부에서 여러 API KEY를 로테이션
- API별 요청 방식이 다르므로 파일을 분리하여 유지보수 용이

#### ai_models
- 요약을 실행하는 모델 추론 코드
- /timeline, /merge 모두에서 공통으로 호출

#### models
- API 요청/응답을 위한 Pydantic 모델 정의
- 백엔드 ↔ AI 서버 사이 통신 포맷 명확히 정의

#### utils
- 작은, 기타 함수들을 보관한다. (logger.py, date_utils.py)
- 로깅 → 프로젝트가 커질수록 필수. print()만으로는 고도화된 필터링 등이 어렵다
- 날짜 → 날짜 계산이 잦을 예정이므로, 함수로 분리하여 안정적으로 재사용

---

## 오늘의 도전 과제와 해결 방법
- 도전 과제 1: 모델 조사
- 도전 과제 2: 아키텍처 모듈화

## 오늘의 회고
- 설계가 진행이 빨라져서 다행이다.
- 용어 정의, 역할 분담 등 명확한 계획이 있을 때와 없을 때의 차이가 매우 크다
  
## 참고 자료 및 링크
- [뉴스 타임라인 서비스 AI팀 위키](https://github.com/100-hours-a-week/18-team-timeline-wiki/wiki/AI-Wiki)
