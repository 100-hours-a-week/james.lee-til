# 날짜: 2025-03-10

## 새로 배운 내용
### 주제 1: NLP
- 번역, 감정 분석, 키워드 추출, 채팅봇 등 자연어를 처리하는 작업
- 컴퓨터가 자연어를 처리하기 위해, 단어의 벡터화가 필요하다
- Bag of Words(BoW): 단어들의 문장 내 등장 빈도를 벡터로 표현
  tf-idf: 다른 문서에서의 빈도도 확인, 항상 자주 등장하는 관사 등에 패널티 부여
#### Word2Vec
- 단어의 벡터화를 One-hot이 아닌, 연산 가능한 방식으로 이루어 낸 방법
- Continuous BoW: 주변 단어 벡터를 보고 중심 단어 벡터를 예측하도록 학습
- Skip-gram: 중심 단어 벡터를 보고 주변 단어 벡터를 예측하도록 학습

### 주제 2: Sequential Model
#### RNN
- 시간의 흐름에 따른 Sequence가 Input으로 차례차례 들어온다
- Hidden state가 이전 정보들을 누적해서 담고 있다
- Vanishing Gradient 때문에 Long-term dependency 학습이 어렵다

#### LSTM
- Input gate, Forget gate를 통해 어떤 정보를 잊을지 고른다
- Cell state를 추가로 도입, 오래되면 잊는 게 아니라 중요성이 떨어지는 것을 잊는다
- Long-term dependency를 좀 더 잘 파악한다

### 주제 3: Text CNN
- 워드 임베딩 이후 Convolution filter를 이용
- Sequential model과 달리 병렬 처리가 가능
- 그러나 Window 범위 바깥 단어들의 관계는 파악 어렵다

### 주제 4: GAN
- Generator: 데이터를 생성하는 모델
- Discriminator: 데이터의 진위를 판별하는 모델
- 두 모델이 경쟁하며 학습을 반복해, 결과적으로 데이터 생성이 가능해진다
- Noise vector는 랜덤의 시드값같은 것으로, 연산을 할 수 있는 성질이 있다

### 주제 5: Transformer
- aaa

## 오늘의 도전 과제와 해결 방법
- 도전 과제 1: aa

## 오늘의 회고
- aa

## 참고 자료 및 링크
- [Transformer 논문 리뷰](https://m0n0rail.tistory.com/entry/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Attention-Is-All-You-Need)
