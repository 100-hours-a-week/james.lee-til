# 날짜: 2025-02-24

## 새로 배운 내용
### 주제 1: Activation function
- 선형 함수는 아무리 합성해도 선형이다. 여기에 비선형성을 더해주는 역할
- Sigmoid: (0,1)의 값을 가지며, Vanishing gradient 문제
- tanh: (-1,1)의 값을 가지며, zero-centered라 좀 더 낫다
- ReLU: max(0,x)라 계산이 간단하고, Vanishing gradient를 해결

### 주제 2: Neural Network
- Feed Forward: 순전파 과정
- Loss function: 오차를 계산
- Backpropagation: 가중치 업데이트를 얼마나 할지 계산
- Optimizer: 실제로 가중치를 갱신, Adam을 많이 사용한다
- Batch size: 계산의 효율성을 위해 사용, 보통 2의 제곱수

## 오늘의 도전 과제와 해결 방법
- 도전 과제 1: aa

## 오늘의 회고
- aa

## 참고 자료 및 링크
- aa
