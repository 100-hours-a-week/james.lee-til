# 날짜: 2025-02-24

## 새로 배운 내용
### 주제 1: Activation function
- 선형 함수는 아무리 합성해도 선형이다. 여기에 비선형성을 더해주는 역할
- Sigmoid: (0,1)의 값을 가지며, Vanishing gradient 문제
- tanh: (-1,1)의 값을 가지며, zero-centered라 좀 더 낫다
- ReLU: max(0,x)라 계산이 간단하고, Vanishing gradient를 해결

### 주제 2: Neural Network
- Feed Forward: 순전파 과정
- Loss function: 오차를 계산
- Backpropagation: 가중치 업데이트를 얼마나 할지 계산
- Optimizer: 실제로 가중치를 갱신, Adam을 많이 사용한다
- Batch size: 계산의 효율성을 위해 사용, 보통 2의 제곱수

### 주제 3: Optimizer
- Adaptive: 자주 업데이트되는 가중치일수록 학습률을 낮춘다. Gradient norm의 합계를 Penalty처럼 사용
- Momentum: 관성과 같은 효과로 Local minima를 피한다. Exponential Moving Average를 사용
- Adam: Adaptive Momentum의 약자로, 두 가지 방법을 모두 사용하여 최적화를 한다

### 주제 4: CNN
- 이미지의 경우 공간 정보를 보존해야 하고, 또 크기가 크다
- 이를 다루기 위해 필터를 사용한다

## 오늘의 도전 과제와 해결 방법
- 도전 과제 1: aa

## 오늘의 회고
- aa

## 참고 자료 및 링크
- aa
